{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MVjenA3xeVW3",
        "obVbKfIMg0Hh",
        "nvvepamWhXh5",
        "enkNyx_7iFla"
      ],
      "authorship_tag": "ABX9TyO9ennRUhp0q8xDW9xu+Ku3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MAY2704/ML_usecases/blob/main/Test_data_generate/Synthetic_test_data_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This is the preprocess stage to load necessary pre-requisites**\n",
        "# **ðŸ›«**"
      ],
      "metadata": {
        "id": "FSev-yTtdZ5M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Hyperparameters (adjust as needed)\n",
        "latent_dim = 100  # Dimension of the latent space\n",
        "batch_size = 32  # Number of transactions per training batch\n",
        "epochs = 10  # Number of training epochs"
      ],
      "metadata": {
        "id": "UyHFTQoUeCtt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **This is an example stage to load sample test data for patterns**"
      ],
      "metadata": {
        "id": "MVjenA3xeVW3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bv2G0EyGj4vf"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# Local Test Data (replace with your actual data)\n",
        "local_test_data = [\n",
        "    {\"amount\": 100, \"category\": \"debit\", \"date\": \"2024-06-15\", \"customer_id\": \"anonymized_id1\"},\n",
        "    {\"amount\": 50, \"category\": \"credit\", \"date\": \"2024-06-12\", \"customer_id\": \"anonymized_id2\"},\n",
        "    {\"amount\": 70, \"category\": \"credit\", \"date\": \"2024-06-13\", \"customer_id\": \"anonymized_id3\"},\n",
        "    {\"amount\": 55, \"category\": \" debit\", \"date\": \"2024-06-17\", \"customer_id\": \"anonymized_id4\"},\n",
        "    {\"amount\": 45, \"category\": \"credit\", \"date\": \"2024-06-08\", \"customer_id\": \"anonymized_id5\"},\n",
        "    {\"amount\": 56, \"category\": \"credit\", \"date\": \"2024-06-09\", \"customer_id\": \"anonymized_id6\"},\n",
        "    {\"amount\": 90, \"category\": \"credit\", \"date\": \"2024-06-11\", \"customer_id\": \"anonymized_id7\"},\n",
        "    {\"amount\": 189, \"category\": \"credit\", \"date\": \"2024-06-14\", \"customer_id\": \"anonymized_id8\"},\n",
        "    {\"amount\": 1000, \"category\": \"credit\", \"date\": \"2024-06-15\", \"customer_id\": \"anonymized_id9\"},\n",
        "    {\"amount\": 978, \"category\": \"credit\", \"date\": \"2024-06-11\", \"customer_id\": \"anonymized_id10\"},\n",
        "    {\"amount\": 45, \"category\": \"credit\", \"date\": \"2024-06-19\", \"customer_id\": \"anonymized_id11\"},\n",
        "    {\"amount\": 123, \"category\": \"credit\", \"date\": \"2024-06-20\", \"customer_id\": \"anonymized_id12\"},\n",
        "    {\"amount\": 78, \"category\": \"debit\", \"date\": \"2024-06-21\", \"customer_id\": \"anonymized_id13\"},\n",
        "    {\"amount\": 33, \"category\": \"credit\", \"date\": \"2024-06-22\", \"customer_id\": \"anonymized_id14\"},\n",
        "    {\"amount\": 87, \"category\": \"credit\", \"date\": \"2024-06-17\", \"customer_id\": \"anonymized_id15\"},\n",
        "    {\"amount\": 11, \"category\": \"debit\", \"date\": \"2024-06-16\", \"customer_id\": \"anonymized_id16\"},\n",
        "    {\"amount\": 15, \"category\": \"credit\", \"date\": \"2024-06-15\", \"customer_id\": \"anonymized_id17\"},\n",
        "    {\"amount\": 19, \"category\": \"debit\", \"date\": \"2024-06-14\", \"customer_id\": \"anonymized_id18\"},\n",
        "    {\"amount\": 20, \"category\": \"credit\", \"date\": \"2024-06-12\", \"customer_id\": \"anonymized_id19\"},\n",
        "    {\"amount\": 21, \"category\": \"debit\", \"date\": \"2024-06-12\", \"customer_id\": \"anonymized_id20\"},\n",
        "    {\"amount\": 78, \"category\": \"credit\", \"date\": \"2024-06-15\", \"customer_id\": \"anonymized_id21\"},\n",
        "    {\"amount\": 45, \"category\": \"credit\", \"date\": \"2024-06-11\", \"customer_id\": \"anonymized_id22\"},\n",
        "    {\"amount\": 18, \"category\": \"credit\", \"date\": \"2024-06-03\", \"customer_id\": \"anonymized_id23\"},\n",
        "    {\"amount\": 156, \"category\": \"credit\", \"date\": \"2024-06-10\", \"customer_id\": \"anonymized_id24\"},\n",
        "    {\"amount\": 198, \"category\": \"debit\", \"date\": \"2024-06-21\", \"customer_id\": \"anonymized_id25\"},\n",
        "    {\"amount\": 189, \"category\": \"credit\", \"date\": \"2024-06-22\", \"customer_id\": \"anonymized_id26\"},\n",
        "    {\"amount\": 100, \"category\": \"credit\", \"date\": \"2024-06-12\", \"customer_id\": \"anonymized_id27\"},\n",
        "    {\"amount\": 198, \"category\": \"debit\", \"date\": \"2024-06-11\", \"customer_id\": \"anonymized_id28\"},\n",
        "    {\"amount\": 10, \"category\": \"credit\", \"date\": \"2024-06-15\", \"customer_id\": \"anonymized_id29\"},\n",
        "    {\"amount\": 76, \"category\": \"credit\", \"date\": \"2024-06-14\", \"customer_id\": \"anonymized_id30\"},\n",
        "    {\"amount\": 34, \"category\": \"credit\", \"date\": \"2024-06-09\", \"customer_id\": \"anonymized_id31\"},\n",
        "    {\"amount\": 39, \"category\": \"debit\", \"date\": \"2024-06-08\", \"customer_id\": \"anonymized_id32\"}\n",
        "]\n",
        "def load_and_preprocess_data(local_data):\n",
        "    # Convert local data to a Pandas DataFrame (optional)\n",
        "    data = pd.DataFrame(local_data)\n",
        "\n",
        "    # Handle missing values (example: fill with mean)\n",
        "    # data = data.fillna(data.mean())  # Or other appropriate imputation techniques\n",
        "\n",
        "    # Deal with outliers (example: capping or removing)\n",
        "    for col in data.columns:\n",
        "        if pd.api.types.is_numeric_dtype(data[col]):\n",
        "            q1 = data[col].quantile(0.25)\n",
        "            q3 = data[col].quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            data.loc[data[col] < (q1 - 1.5 * iqr), col] = q1 - 1.5 * iqr\n",
        "            data.loc[data[col] > (q3 + 1.5 * iqr), col] = q3 + 1.5 * iqr\n",
        "\n",
        "    # Encode categorical features (example: one-hot encoding)\n",
        "    categorical_cols = [col for col in data.columns if data[col].dtype == 'object']\n",
        "    data = pd.get_dummies(data, columns=categorical_cols, drop_first=True)\n",
        "    # Scale numerical features (example: standardization)\n",
        "    from sklearn.preprocessing import StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    data[data.columns[data.dtypes != 'object']] = scaler.fit_transform(data[data.columns[data.dtypes != 'object']])\n",
        "\n",
        "    # Return features\n",
        "    features = data  # Assuming all columns are features for GAN training\n",
        "\n",
        "    return features\n",
        "\n",
        "# Load and preprocess data\n",
        "real_transactions = load_and_preprocess_data(local_test_data)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define the generator model to generate synthetic test data**"
      ],
      "metadata": {
        "id": "obVbKfIMg0Hh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Define the Generator Network\n",
        "def build_generator():\n",
        "  model = tf.keras.Sequential([\n",
        "      layers.Dense(128, activation='relu', input_shape=(latent_dim,)),\n",
        "      layers.Dense(256, activation='relu'),\n",
        "      layers.Dense(512, activation='relu'),\n",
        "      layers.Dense(real_transactions.shape[1], activation='sigmoid'),  # Adjust output size based on features\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "\n",
        "generator = build_generator()  # Create the generator object here\n",
        "\n"
      ],
      "metadata": {
        "id": "ZNxd6EwyhIMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Define discriminator model to detect issues with generated test data**"
      ],
      "metadata": {
        "id": "nvvepamWhXh5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Discriminator Network\n",
        "def build_discriminator():\n",
        "    model = tf.keras.Sequential([\n",
        "        layers.Dense(256, activation='relu', input_shape=(real_transactions.shape[1],)),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(512, activation='relu'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(1, activation='sigmoid'),\n",
        "    ])\n",
        "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "    return model\n",
        "\n",
        "\n",
        "discriminator = build_discriminator()\n",
        "\n"
      ],
      "metadata": {
        "id": "6-VnBmjEhfbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train the generator & discriminator, so discriminator can 'fight' with generator to detect antipatterns in generated test data**\n",
        "\n",
        "# **Generator vs Discriminator ðŸ¤œðŸ¤›**"
      ],
      "metadata": {
        "id": "Ps4Nu7W-hxS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the combined GAN model for training (same as before)\n",
        "discriminator.trainable = False\n",
        "gan_model = tf.keras.Sequential([generator, discriminator])\n",
        "gan_model.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "\n",
        "def generate_batches(data, batch_size):\n",
        "  # Create an empty list to store batches\n",
        "  batches = []\n",
        "\n",
        "  # Loop through the data in chunks of batch_size\n",
        "  for i in range(0, len(data), batch_size):\n",
        "    # Get a batch of data\n",
        "    batch = data[i:i + batch_size]\n",
        "    # Append the batch to the list\n",
        "    batches.append(batch)\n",
        "\n",
        "  return batches\n",
        "def train_gan(epochs):\n",
        "  # Noise vector dimension matching latent space\n",
        "  noise_dim = latent_dim\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    # For each epoch\n",
        "    batch_count = 0\n",
        "    for real_transactions_batch in generate_batches(real_transactions, batch_size):\n",
        "      # Generate random noise for the generator\n",
        "      noise = tf.random.normal(shape=(batch_size, noise_dim))\n",
        "      # Generate synthetic transactions using the noise\n",
        "      generated_transactions = generator(noise)\n",
        "      # Train the Discriminator: Maximize ability to distinguish real from fake\n",
        "\n",
        "      # Define real labels (ones) for real transactions\n",
        "      real_labels = tf.ones((batch_size, 1))\n",
        "      # Reshape real_labels to match batch size (if needed)\n",
        "      real_labels = tf.reshape(real_labels[:batch_size], (-1, 1))  # Select first 'batch_size' labels and reshape\n",
        "      print(\"real_transactions_batch:\", real_transactions_batch.shape)\n",
        "      print(\"real_labels:\", real_labels.shape)\n",
        "      # Concatenate along the feature dimension (usually the last dimension)\n",
        "      concatenated_data = tf.concat([real_transactions_batch, real_labels], axis=-1)\n",
        "      # Define fake labels (zeros) for generated transactions\n",
        "      fake_labels = tf.zeros((batch_size, 1))\n",
        "      print(\"fake_labels:\", fake_labels.shape)\n",
        "\n",
        "      # Train the discriminator on real transactions\n",
        "      discriminator_loss_real = discriminator.train_on_batch(real_transactions_batch, real_labels)\n",
        "\n",
        "\n",
        "      # Train the discriminator on synthetic transactions\n",
        "      generated_labels = tf.zeros((batch_size, 1))  # The discriminator tries to identify these as fake\n",
        "      discriminator_loss_fake = discriminator.train_on_batch(generated_transactions, generated_labels)\n",
        "\n",
        "      # Calculate total discriminator loss\n",
        "      discriminator_loss = 0.5 * (discriminator_loss_real + discriminator_loss_fake)\n",
        "\n",
        "      # Train the generator: Maximize the discriminator's mistakes\n",
        "      noise = tf.random.normal(shape=(batch_size, noise_dim))\n",
        "      gan_loss = gan_model.train_on_batch(noise, tf.ones((batch_size, 1)))\n",
        "\n",
        "      print(f'Epoch: {epoch}, Discriminator Loss: {discriminator_loss}, GAN Loss: {gan_loss}')\n",
        "\n",
        "      synthetic_samples = generator.predict(np.random.rand(1000, latent_dim))\n",
        "      return synthetic_samples\n",
        "\n",
        "\n",
        "synthetic_samples = train_gan(epochs)\n",
        "synthetic_samples_df = pd.DataFrame(synthetic_samples)\n",
        "\n",
        "# Define a function to reverse the scaling\n",
        "def reverse_scaling(scaled_data, original_data):\n",
        "    scaler = StandardScaler()\n",
        "    scaler.fit_transform(original_data)\n",
        "    return scaler.inverse_transform(scaled_data)\n",
        "\n",
        "\n",
        "def reverse_encoding(encoded_data, original_data):\n",
        "    categorical_cols = [col for col in original_data.columns if original_data[col].dtype == 'object']\n",
        "    for col in categorical_cols:\n",
        "        encoded_cols = [c for c in encoded_data.columns if c.startswith(col)]\n",
        "        encoded_data[col] = encoded_data[encoded_cols].idxmax(axis=1)\n",
        "        encoded_data[col] = encoded_data[col].str.replace(col+'_', '')\n",
        "        encoded_data = encoded_data.drop(columns=encoded_cols)\n",
        "    return encoded_data\n",
        "# Define a function to do the postprocessing\n",
        "def postprocess_data(synthetic_samples_df, real_transactions):\n",
        "    # Reverse the scaling\n",
        "    reverse_scaled_data = reverse_scaling(synthetic_samples_df, real_transactions)\n",
        "    # Reverse the encoding\n",
        "    postprocessed_data = reverse_encoding(reverse_scaled_data, real_transactions)\n",
        "    return postprocessed_data\n",
        "postprocessed_samples = postprocess_data(synthetic_samples_df, real_transactions)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EH43Xzvrh-mv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33b808ef-79fd-441a-9199-046b2c1130a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "real_transactions_batch: (32, 48)\n",
            "real_labels: (32, 1)\n",
            "fake_labels: (32, 1)\n",
            "Epoch: 0, Discriminator Loss: 0.7802577018737793, GAN Loss: 0.5898131728172302\n",
            "32/32 [==============================] - 0s 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Run the model and print synthetic samples**"
      ],
      "metadata": {
        "id": "enkNyx_7iFla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Synthetic test data samples are created\")\n",
        "\n",
        "print(postprocessed_samples)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GACKGcfniK__",
        "outputId": "28bf9a7e-61e7-4461-a21f-7a907e95c044"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic test data samples are created\n",
            "[[0.5774429  0.5150787  0.47595927 ... 0.5540758  0.460086   0.50050735]\n",
            " [0.56590027 0.53782755 0.47319967 ... 0.5671035  0.48615906 0.4954995 ]\n",
            " [0.5383592  0.50552714 0.48761857 ... 0.561852   0.46817887 0.501293  ]\n",
            " ...\n",
            " [0.5504726  0.50230396 0.49552804 ... 0.5517026  0.46639284 0.48651636]\n",
            " [0.5503055  0.5111661  0.49693635 ... 0.54041886 0.47098148 0.48282596]\n",
            " [0.55840707 0.5270952  0.4649615  ... 0.5473233  0.48966175 0.4898357 ]]\n"
          ]
        }
      ]
    }
  ]
}